from langchain.document_loaders.base import BaseLoader
from abc import ABC
from typing import Any, Dict, Iterator, List, Mapping, Optional, Sequence, Union,Iterable
from langchain.docstore.document import Document
from langchain.document_loaders.base import BaseBlobParser
from langchain.document_loaders.blob_loaders import Blob
import numpy as np
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores.pgvector import PGVector
from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import RedisChatMessageHistory
from langchain.llms import LlamaCpp
from langchain.chains import ConversationChain, LLMChain, ConversationalRetrievalChain, RetrievalQA
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.prompts.prompt import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import MongoDBChatMessageHistory
from langchain.vectorstores.pgvector import PGVector
import queue
from langchain.callbacks.manager import AsyncCallbackManager
import threading
from pydantic import BaseModel
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse

class ThreadedGenerator:
    def __init__(self):
        self.queue = queue.Queue()

    def __iter__(self):
        return self

    def __next__(self):
        item = self.queue.get()
        if item is StopIteration: raise item
        return item

    def send(self, data):
        self.queue.put(data)

    def close(self):
        self.queue.put(StopIteration)
        

class ChainStreamHandler(StreamingStdOutCallbackHandler):
    def __init__(self, gen):
        super().__init__()
        self.gen = gen

    def on_llm_new_token(self, token: str, **kwargs):
        self.gen.send(token)
        

def llm_thread(g, query):
    try:
        template = """Use the following pieces of information to answer the user's question.
        If you don't know the answer, just say that you don't know, don't try to make up an answer.

        Context: {context}
        Question: {question}

        Only return the helpful answer below and nothing else.
        Helpful answer:
        """

        prompt = PromptTemplate(input_variables=["context",  "question"], template=template)
        embeddings = HuggingFaceEmbeddings(model_name=r"D:\Workspace\DocChat\models\embedding\all-mpnet-base-v2", model_kwargs={"device": 'cuda'})
        CONNECTION_STRING = "postgresql+psycopg2://postgres:admin@localhost:5432/vector_db"
        COLLECTION_NAME = 'document_vector'
        db = PGVector(
            collection_name=COLLECTION_NAME,
            connection_string=CONNECTION_STRING,
            embedding_function=embeddings,
        )
        llm = LlamaCpp(model_path='D:/Workspace/PrivateGPTLangchain/models/llm/mistral-7b-v0.1.Q4_0.gguf',
               n_ctx=10000,
               max_tokens=250,
               n_batch=512,
               callbacks=AsyncCallbackManager([ChainStreamHandler(g)]),
               n_gpu_layers=-1)

        qa = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=db.as_retriever(),
                return_source_documents=True,
                chain_type_kwargs={'prompt': prompt}
        )
        result = qa(query)
        ans = result

    finally:
        g.close()

def chain(query):
    g = ThreadedGenerator()
    threading.Thread(target=llm_thread, args=(g, query)).start()
    return g

class ChainRequest(BaseModel):
    query: str
    

app = FastAPI()
# CORS configuration
origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/chain")
async def _chain(request: ChainRequest):
    print(request)
    print(request.query)
    gen = chain(query=request.query)
    return StreamingResponse(gen, media_type="text/plain")

#if __name__ == '__main__':

    
    # embeddings = HuggingFaceEmbeddings(model_name=r"D:\Workspace\DocChat\models\embedding\all-mpnet-base-v2", model_kwargs={"device": 'cuda'})

    # CONNECTION_STRING = "postgresql+psycopg2://postgres:admin@localhost:5432/vector_db"
    # COLLECTION_NAME = 'document_vector'
    # db = PGVector(
    #     collection_name=COLLECTION_NAME,
    #     connection_string=CONNECTION_STRING,
    #     embedding_function=embeddings,
    # )
    # # template = """You are helpful information giving QA System and make sure you don't answer anything 
    # # # not related to following context. You are always provide useful information & details available in the given context. Use the following pieces of context to answer the question at the end. 
    # # # If you don't know the answer, just say that you don't know, don't try to make up an answer. 

    # # # {context}

    # # # Question: {question}
    # # # Helpful Answer:"""
    # template = """Use the following pieces of information to answer the user's question.
    # If you don't know the answer, just say that you don't know, don't try to make up an answer.

    # Context: {context}
    # Question: {question}

    # Only return the helpful answer below and nothing else.
    # Helpful answer:
    # """

    # prompt = PromptTemplate(
    # input_variables=["context",  "question"], template=template)
    # connection_string = f"mongodb://localhost:27017/"
    # history = MongoDBChatMessageHistory(
    #     connection_string=connection_string, session_id="user_id"
    # )
    # # Initialize the ConversationBufferMemory
    # memory = ConversationBufferMemory(
    #     memory_key="chat_history",          # Ensure this matches the key used in chain's prompt template
    #     chat_memory=history,   # Pass the RedisChatMessageHistory instance
    #     return_messages=True,          # Does your prompt template expect a string or a list of Messages?
    #     k = 5,
    #     output_key='answer'
    # )
    # llm = LlamaCpp(model_path='D:/Workspace/PrivateGPTLangchain/models/llm/mistral-7b-v0.1.Q4_0.gguf',
    #            n_ctx=10000,
    #            max_tokens=250,
    #            n_batch=512,
    #            callbacks=[StreamingStdOutCallbackHandler()],
    #            n_gpu_layers=-1)
    # # qa = RetrievalQA.from_chain_type(
    # # llm=llm,
    # # chain_type="stuff",
    # # retriever=db.as_retriever(),
    # # return_source_documents=True,
    # # memory=memory,
    # # chain_type_kwargs={'prompt': prompt}
    # # )
    # qa = ConversationalRetrievalChain.from_llm(
    #     llm=llm,
    #     memory=memory,
    #     chain_type="stuff",
    #     retriever=db.as_retriever(),
    #     return_source_documents=True,
    #     combine_docs_chain_kwargs={"prompt": prompt},
    #     verbose=False)
    # while True:
    #     query = input()
    #     result = qa({"question": query})
    #     result


docker load -i D:\Software\Docker_Images\pgvector-containerimage.tar
(base) D:\Software\Docker_Images>docker run --name pgvector-image -e POSTGRES_PASSWORD=admin -p 5432:5432 -d ankane/pgvector:latest
77e30db6c3b97855737d01fac20f5fa70c30c73fac3a28f5744b588ab443be8e
docker start 77e30db6c3b97855737d01fac20f5fa70c30c73fac3a28f5744b588ab443be8e


docker pull redis/redis-stack:latest
docker save --output redis-containerimage.tar redis/redis-stack:latest
(base) D:\Software\Docker_Images>docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest
a0a3387e6f1bc2d43c51c80067a32c59da60e3e99f14dc071c622df270f92c37

https://github.com/ElmiraGhorbani/redis-chatgpt

(base) D:\Software\Docker_Images>docker run -d -p 27017:27017 --name test-mongo mongo:latest
4a37df507aaf78f2b0696592c2f4b838f4e34c57233e315358aacc55dccf8513

uvicorn main:app --host 0.0.0.0 --port 8443
